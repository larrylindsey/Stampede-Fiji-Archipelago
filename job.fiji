#!/bin/bash
#
#-----------------------------------------------------------------------------
# This Stampede job script is designed to create a vnc session on 
# visualization nodes through the SLURM batch system. Once the job
# is scheduled, check the output of your job (which by default is
# stored in your home directory in a file named vncserver.out)
# and it will tell you the port number that has been setup for you so
# that you can attach via a separate VNC client to any Stampede login 
# node (e.g., login1.stampede.tacc.utexas.edu).
#
# Note that for security, we recommend setting up a tunneled VNC
# session in order to connect via a client (more information on doing
# this is available at the User Guide link below).  Once you connect,
# you should see a single xterm running which you can use to launch
# any X application (eg. Paraview or VisIt) 
#
# Note: you can fine tune the SLURM submission variables below as
# needed.  Typical items to change are the runtime limit, location of
# the job output, and the allocation project to submit against (it is
# commented out for now, but is required if you have multiple
# allocations).  
#
# To submit the job, issue: "qsub /share/doc/slurm/job.vnc" 
#
# For more information, please consult the User Guide at: 
#
# http://www.tacc.utexas.edu/user-services/user-guides/stampede-user-guide
#-----------------------------------------------------------------------------
#
#SBATCH -J vncserver                  # Job name
#SBATCH -o vncserver.out              # Name of stdout output file (%j expands to jobId)
#SBATCH -p vis                        # Queue name
#SBATCH -N 1                          # Total number of nodes requested (16 cores/node)
#SBATCH -n 16                         # Total number of mpi tasks requested
#SBATCH -t 04:00:00                   # Run time (hh:mm:ss) - 4 hours

#--------------------------------------------------------------------------
# ---- You normally should not need to edit anything below this point -----
#--------------------------------------------------------------------------

echo job $JOB_ID execution at: `date`

# our node name
NODE_HOSTNAME=`hostname -s`
echo "running on node $NODE_HOSTNAME"

# VNC server executable
VNCSERVER_BIN=`which vncserver`
echo "using default VNC server $VNCSERVER_BIN"

# set memory limits to 95% of total memory to prevent node death
NODE_MEMORY=`free -k | grep ^Mem: | awk '{ print $2; }'`
NODE_MEMORY_LIMIT=`echo "0.95 * $NODE_MEMORY / 1" | bc`
ulimit -v $NODE_MEMORY_LIMIT -m $NODE_MEMORY_LIMIT
echo "memory limit set to $NODE_MEMORY_LIMIT kilobytes"

# set wayness, used for ibrun
WAYNESS=`echo $PE | perl -pe 's/([0-9]+)way/\1/;'`
echo "set wayness to $WAYNESS"

# Check whether a vncpasswd file exists.  If not, complain and exit.
if [ \! -e $HOME/.vnc/passwd ] ; then
	echo 
	echo "=================================================================="
	echo "   You must run 'vncpasswd' once before launching a vnc session"
	echo "=================================================================="
	echo
	exit 1
fi

# launch VNC session
VNC_DISPLAY=`$VNCSERVER_BIN $@ 2>&1 | grep desktop | awk -F: '{print $3}'`
echo "got VNC display :$VNC_DISPLAY"

# todo: make sure this is a valid display, and that it is 1 or 2, since those are the only displays forwarded
# using our iptables scripts

if [ x$VNC_DISPLAY == "x" ]; then
    echo 
    echo "===================================================="
    echo "   Error launching vncserver: $VNCSERVER"
    echo "   Please submit a ticket to the TACC User Portal"
    echo "   http://portal.tacc.utexas.edu/"
    echo "===================================================="
    echo
    exit 1
fi

LOCAL_VNC_PORT=`expr 5900 + $VNC_DISPLAY`
echo "local (compute node) VNC port is $LOCAL_VNC_PORT"

# the largemem queue has traditional numbering, other queues have row-node numbering
if [ $SLURM_QUEUE == "largemem" ]; then
    LOGIN_VNC_PORT="$VNC_DISPLAY`echo $NODE_HOSTNAME | perl -ne 'print $1.$2 if /c\d(\d\d)-\d(\d\d)/;'`"
else
    LOGIN_VNC_PORT="$VNC_DISPLAY`echo $NODE_HOSTNAME | perl -ne 'print $1.$2.$3 if /c\d(\d\d)-(\d)\d(\d)/;'`"
fi

#LOGIN_VNC_PORT="$VNC_DISPLAY`echo $NODE_HOSTNAME | perl -ne 'print 59$2 if /c\d(\d\d)-\d(\d\d)/;'`"
echo "got login node VNC port $LOGIN_VNC_PORT"

# create reverse tunnel port to login nodes.  Make one tunnel for each login so the user can just
# connect to stampede.tacc
for i in `seq 4`; do
    ssh -f -g -N -R $LOGIN_VNC_PORT:$NODE_HOSTNAME:$LOCAL_VNC_PORT login$i
done
echo "Created reverse ports on Stampede logins"

echo "Your VNC server is now running!"
echo "To connect via VNC client:  SSH tunnel port $LOGIN_VNC_PORT to stampede.tacc.utexas.edu:$LOGIN_VNC_PORT"
echo "                            Then connect to localhost::$LOGIN_VNC_PORT"
#echo
#echo "OR:                         Connect directly to login1.longhorn.tacc.utexas.edu::$LOGIN_VNC_PORT"
#echo

# set display for X applications
export DISPLAY=":$VNC_DISPLAY"


# Warn the user when their session is about to close
# see if the user set their own runtime
#TACC_RUNTIME=`qstat -j $JOB_ID | grep h_rt | perl -ne 'print $1 if /h_rt=(\d+)/'`  # qstat returns seconds
TACC_RUNTIME=`squeue -l -j $SLURM_JOB_ID | grep $SLURM_QUEUE | awk '{print $7}'` # squeue returns HH:MM:SS
if [ x"$TACC_RUNTIME" == "x" ]; then
	TACC_Q_RUNTIME=`sinfo -p $SLURM_QUEUE | grep -m 1 $SLURM_QUEUE | awk '{print $3}'`
	if [ x"$TACC_Q_RUNTIME" != "x" ]; then
		# pnav: this assumes format hh:dd:ss, will convert to seconds below
		#       if days are specified, this won't work
		TACC_RUNTIME=$TACC_Q_RUNTIME
	fi
fi

if [ x"$TACC_RUNTIME" != "x" ]; then
	# there's a runtime limit, so warn the user when the session will die
	# give 5 minute warning for runtimes > 5 minutes
        H=$((`echo $TACC_RUNTIME | awk -F: '{print $1}'` * 3600))		
        M=$((`echo $TACC_RUNTIME | awk -F: '{print $2}'` * 60))		
        S=`echo $TACC_RUNTIME | awk -F: '{print $3}'`
        TACC_RUNTIME_SEC=$(($H + $M + $S))
        
	if [ $TACC_RUNTIME_SEC -gt 300 ]; then
        	TACC_RUNTIME_SEC=`expr $TACC_RUNTIME_SEC - 300`
        	sleep $TACC_RUNTIME_SEC && echo "$USER's VNC session on $VNC_DISPLAY will end in 5 minutes.  Please save your work now." | wall &
        fi
fi

# we need vglclient to run to have graphics across multi-node jobs
vglclient >& /dev/null &
VGL_PID=$!

# run an xterm for the user; execution will hold here
xterm -r -ls -geometry 80x24+10+10 -title '*** Exit this window to kill your VNC server ***'

# job is done!

echo "Killing VGL client"
kill $VGL_PID

echo "Killing VNC server" 
vncserver -kill $DISPLAY

# wait a brief moment so vncserver can clean up after itself
sleep 1

echo job $_SLURM_JOB_ID execution finished at: `date`
